{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67ba01fb-0147-40e3-8416-08db8743abcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<absl.flags._flagvalues.FlagHolder at 0x7f52886faeb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, './src')\n",
    "\n",
    "from absl import flags, app\n",
    "flags.DEFINE_string('f', '', 'kernel')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c769c28d-356e-429c-9a7f-21b56b8d0f0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk2/anaconda3/envs/glom/lib/python3.8/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:35: UnderReviewWarning: The feature generate_power_seq is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  \"lr_options\": generate_power_seq(LEARNING_RATE_CIFAR, 11),\n",
      "/disk2/anaconda3/envs/glom/lib/python3.8/site-packages/pl_bolts/models/self_supervised/amdim/amdim_module.py:93: UnderReviewWarning: The feature FeatureMapContrastiveTask is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  contrastive_task: Union[FeatureMapContrastiveTask] = FeatureMapContrastiveTask(\"01, 02, 11\"),\n",
      "/disk2/anaconda3/envs/glom/lib/python3.8/site-packages/pl_bolts/losses/self_supervised_learning.py:234: UnderReviewWarning: The feature AmdimNCELoss is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  self.nce_loss = AmdimNCELoss(tclip)\n",
      "/disk2/anaconda3/envs/glom/lib/python3.8/site-packages/pl_bolts/datamodules/experience_source.py:18: UnderReviewWarning: The feature warn_missing_pkg is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  warn_missing_pkg(\"gym\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logtostderr': False, 'alsologtostderr': False, 'log_dir': '', 'v': -1, 'verbosity': -1, 'logger_levels': {}, 'stderrthreshold': 'fatal', 'showprefixforinfo': True, 'run_with_pdb': False, 'pdb_post_mortem': False, 'pdb': False, 'run_with_profiling': False, 'profile_file': None, 'use_cprofile_for_profiling': True, 'only_check_args': False, 'f': '', 'dataset': 'CIFAR10', 'exp_name': 'CIFAR10', 'logger': 'tensorboard', 'patch_size': 4, 'patch_dim': 128, 'batch_size': 1024, 'levels': 2, 'supervise': False, 'image_size': 32, 'conv_image_size': 8, 'n_channels': 3, 'n_classes': 10, 'iters': None, 'denoise_iter': -1, 'dropout': 0.3, 'temperature': 0.07, 'contr_dim': 512, 'mode': 'train', 'learning_rate': 0.05, 'lr_speed': 2000.0, 'ckpt_dir': None, 'seed': 42, 'max_epochs': 10000, 'weight_decay': 0.0005, 'num_workers': 16, 'gpus': [0], 'limit_train': 1.0, 'limit_val': 1.0, 'limit_test': 1.0, 'plot_islands': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from main import main, get_model, get_DataModule, get_trainer, seed_everything #, flags_Agglomerator\n",
    "\n",
    "arg_str = \" --flagfile config/config_CIFAR10.cfg\"\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "FLAGS(arg_str.split(\" \"))\n",
    "# FLAGS.gpus, FLAGS.dataset#, FLAGS.flagfile     # FLAGS = Namespace(**FLAGS.flag_values_dict())\n",
    "\n",
    "# print(FLAGS)\n",
    "print(FLAGS.flag_values_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97593b1a-f1ce-40c7-af68-bbe23d0aa00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk2/anaconda3/envs/glom/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:478: LightningDeprecationWarning: Setting `Trainer(gpus=[0])` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=[0])` instead.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters:  1192426\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = get_model(FLAGS)\n",
    "dm = get_DataModule(FLAGS)\n",
    "trainer = get_trainer(FLAGS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e127478-5fbe-457c-8546-bb19087cc0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                           | Type               | Params\n",
      "----------------------------------------------------------------------\n",
      "0 | image_to_tokens                | Sequential         | 572   \n",
      "1 | contrastive_head               | Sequential         | 528 K \n",
      "2 | classification_head_from_contr | Sequential         | 267 K \n",
      "3 | bottom_up                      | Sequential         | 263 K \n",
      "4 | top_down                       | Sequential         | 131 K \n",
      "5 | attention                      | ConsensusAttention | 0     \n",
      "----------------------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.770     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk2/anaconda3/envs/glom/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                          "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/disk2/anaconda3/envs/glom/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/disk2/anaconda3/envs/glom/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (39) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  81%|████████████████████████████████████████████▋          | 39/48 [01:03<00:14,  1.64s/it, loss=7.62, v_num=15]\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                   | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                      | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 0:  83%|█████████████████████████████████████████████▊         | 40/48 [01:04<00:12,  1.61s/it, loss=7.62, v_num=15]\u001b[A\n",
      "Epoch 0:  85%|██████████████████████████████████████████████▉        | 41/48 [01:04<00:11,  1.58s/it, loss=7.62, v_num=15]\u001b[A\n",
      "Epoch 0:  88%|████████████████████████████████████████████████▏      | 42/48 [01:05<00:09,  1.56s/it, loss=7.62, v_num=15]\u001b[A\n",
      "Epoch 0:  90%|█████████████████████████████████████████████████▎     | 43/48 [01:06<00:07,  1.54s/it, loss=7.62, v_num=15]\u001b[A\n",
      "Epoch 0:  92%|██████████████████████████████████████████████████▍    | 44/48 [01:06<00:06,  1.51s/it, loss=7.62, v_num=15]\u001b[A\n",
      "Epoch 0:  94%|███████████████████████████████████████████████████▌   | 45/48 [01:07<00:04,  1.49s/it, loss=7.62, v_num=15]\u001b[A\n",
      "Epoch 0:  96%|████████████████████████████████████████████████████▋  | 46/48 [01:07<00:02,  1.47s/it, loss=7.62, v_num=15]\u001b[A\n",
      "Epoch 0:  98%|█████████████████████████████████████████████████████▊ | 47/48 [01:08<00:01,  1.45s/it, loss=7.62, v_num=15]\u001b[A\n",
      "Epoch 0: 100%|███████████████████████████████████████████████████████| 48/48 [01:08<00:00,  1.44s/it, loss=7.62, v_num=15]\u001b[A\n",
      "Epoch 1:  81%|████████████████████████████████████████████▋          | 39/48 [01:03<00:14,  1.64s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                   | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                      | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 1:  83%|█████████████████████████████████████████████▊         | 40/48 [01:04<00:12,  1.61s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Epoch 1:  85%|██████████████████████████████████████████████▉        | 41/48 [01:04<00:11,  1.58s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Epoch 1:  88%|████████████████████████████████████████████████▏      | 42/48 [01:05<00:09,  1.56s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Epoch 1:  90%|█████████████████████████████████████████████████▎     | 43/48 [01:06<00:07,  1.54s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Epoch 1:  92%|██████████████████████████████████████████████████▍    | 44/48 [01:06<00:06,  1.51s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Epoch 1:  94%|███████████████████████████████████████████████████▌   | 45/48 [01:07<00:04,  1.50s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Epoch 1:  96%|████████████████████████████████████████████████████▋  | 46/48 [01:07<00:02,  1.48s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Epoch 1:  98%|█████████████████████████████████████████████████████▊ | 47/48 [01:08<00:01,  1.46s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Epoch 1: 100%|███████████████████████████████████████████████████████| 48/48 [01:09<00:00,  1.44s/it, loss=7.58, v_num=15]\u001b[A\n",
      "Epoch 2:  81%|████████████████████████████████████████████▋          | 39/48 [01:04<00:14,  1.64s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                   | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                      | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 2:  83%|█████████████████████████████████████████████▊         | 40/48 [01:04<00:12,  1.62s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Epoch 2:  85%|██████████████████████████████████████████████▉        | 41/48 [01:05<00:11,  1.59s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Epoch 2:  88%|████████████████████████████████████████████████▏      | 42/48 [01:05<00:09,  1.57s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Epoch 2:  90%|█████████████████████████████████████████████████▎     | 43/48 [01:06<00:07,  1.55s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Epoch 2:  92%|██████████████████████████████████████████████████▍    | 44/48 [01:07<00:06,  1.52s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Epoch 2:  94%|███████████████████████████████████████████████████▌   | 45/48 [01:07<00:04,  1.50s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Epoch 2:  96%|████████████████████████████████████████████████████▋  | 46/48 [01:08<00:02,  1.48s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Epoch 2:  98%|█████████████████████████████████████████████████████▊ | 47/48 [01:08<00:01,  1.46s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Epoch 2: 100%|███████████████████████████████████████████████████████| 48/48 [01:09<00:00,  1.44s/it, loss=7.54, v_num=15]\u001b[A\n",
      "Epoch 3:  81%|████████████████████████████████████████████▋          | 39/48 [01:04<00:14,  1.67s/it, loss=7.52, v_num=15]\u001b[A\n",
      "Validation: 0it [00:00, ?it/s]\u001b[A\n",
      "Validation:   0%|                                                                                   | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|                                                                      | 0/9 [00:00<?, ?it/s]\u001b[A\n",
      "Epoch 3:  83%|█████████████████████████████████████████████▊         | 40/48 [01:05<00:13,  1.64s/it, loss=7.52, v_num=15]\u001b[A\n",
      "Epoch 3:  85%|██████████████████████████████████████████████▉        | 41/48 [01:06<00:11,  1.61s/it, loss=7.52, v_num=15]\u001b[A\n",
      "Epoch 3:  88%|████████████████████████████████████████████████▏      | 42/48 [01:06<00:09,  1.59s/it, loss=7.52, v_num=15]\u001b[A\n",
      "Epoch 3:  90%|█████████████████████████████████████████████████▎     | 43/48 [01:07<00:07,  1.56s/it, loss=7.52, v_num=15]\u001b[A\n",
      "Epoch 3:  92%|██████████████████████████████████████████████████▍    | 44/48 [01:07<00:06,  1.54s/it, loss=7.52, v_num=15]\u001b[A\n",
      "Epoch 3:  94%|███████████████████████████████████████████████████▌   | 45/48 [01:08<00:04,  1.52s/it, loss=7.52, v_num=15]\u001b[A"
     ]
    }
   ],
   "source": [
    "trainer.fit(model,dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a06e5ec-f10d-489c-a1d3-3c4b7170b869",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18aa39ba-d813-4ee8-8971-1d9406b62027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d96df48-b9ca-442f-aa24-b42f36ce4efc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37aab67-b041-4381-9aae-8a99d21c0cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glom",
   "language": "python",
   "name": "glom"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
